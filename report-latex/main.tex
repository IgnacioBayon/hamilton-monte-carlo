\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{booktabs}

% \setbeameroption{show notes}

\usetheme{Madrid}
\usecolortheme{default}

%------------------------------------------------------------
%This block of code defines the information to appear in the
%Title page
\title[HMC]
{Hamiltonian Monte Carlo}

\subtitle{MSc in AI --- Probabilistic Methods}

\author[PM]{%
  \texorpdfstring{
    Ignacio Bayón Jimenez-Ugarte\newline
    Sergio Herreros Pérez\newline
    Adrián López-Lanchares Echezarreta  
  }{Ignacio Bayón Jimenez-Ugarte, Sergio Herreros Pérez, Adrián López-Lanchares Echezarreta}
}

\institute[ICAI]{Universidad Pontificia Comillas, ICAI}

\date[08/29/2025]{08/29/2025}

\logo{\includegraphics[height=1cm]{images/Logo_Comillas}}

%End of title page configuration block
%------------------------------------------------------------



%------------------------------------------------------------
%The next block of commands puts the table of contents at the 
%beginning of each section and highlights the current section:

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[currentsection]
  \end{frame}
}
%------------------------------------------------------------


\begin{document}

% ---------- Title ----------
\begin{frame}\titlepage\end{frame}


% ===================== BODY =====================
\section{Motivation and Background}


\subsection{The Sampling Problem}

\begin{frame}{The Sampling Problem}
  
  \textbf{Goal:} draw samples from a target posterior distribution
  \[
    p(\theta \mid \text{data})
    = \frac{p(\text{data} \mid \theta)\, p(\theta)}{Z},
    \quad
    Z = \int p(\text{data} \mid \theta)\, p(\theta)\, d\theta.
  \]
  
  \vspace{0.3cm}
  
  \textbf{Issue:} the normalising constant $Z$ is often intractable.
  
  \vspace{0.3cm}
  
  \textbf{Workaround in MCMC:}
  \begin{itemize}
    \item We only need the unnormalised density
          \[
            \tilde{\pi}(\theta)
            = p(\text{data} \mid \theta)\, p(\theta)
            \propto p(\theta \mid \text{data}).
          \]
    \item MCMC algorithms use ratios such as
          \(
          \tilde{\pi}(\theta') / \tilde{\pi}(\theta)
          \),
          where $Z$ cancels out.
  \end{itemize}
  
  \vspace{0.2cm}
  
  \textbf{Conclusion:} We can sample from the posterior using only the
  \emph{unnormalised} density $\tilde{\pi}(\theta)$.
  
  \note{
    En muchos problemas bayesianos no podemos calcular la constante de normalización de la distribución posterior, lo que impide muestrear directamente de ella. Los métodos MCMC resuelven esto trabajando solo con la densidad no normalizada, aprovechando que los cocientes de densidades eliminan la constante intractable. Así, el reto no es la evaluación de la posterior, sino cómo explorar eficazmente el espacio de parámetros.
  }
\end{frame}


\subsection{The Metropolis-Hastings Algorithm}

\begin{frame}{The Metropolis-Hastings Algorithm}
  
  \textbf{Goal:} construct a Markov chain whose stationary distribution is the
  target posterior $\pi(\theta)$ (known up to a normalising constant).
  
  \vspace{0.2cm}
  
  \textbf{Algorithm (one iteration):}
  \begin{enumerate}
    \item At current state $\theta$, draw a proposal
          \[
            \theta' \sim q(\theta' \mid \theta) \quad (q\text{: proposal distribution}).
          \]
    \item Compute the acceptance probability
          \[
            \alpha =
            \min\left(
            1,\;
            \frac{\tilde{\pi}(\theta')\, q(\theta \mid \theta')}
              {\tilde{\pi}(\theta)\, q(\theta' \mid \theta)}
            \right).
          \]
    \item With probability $\alpha$, move to $\theta'$;
          otherwise, stay at $\theta$.
  \end{enumerate}
  
  \vspace{0.2cm}
  
  \textbf{Key properties:}
  \begin{itemize}
    \item Ensures $\pi(\theta)$ is the stationary distribution.
    \item Works with the \emph{unnormalised} density $\tilde{\pi}(\theta)$.
    \item Very general: any proposal $q$ is allowed (almost).
  \end{itemize}
  
  \note{
    Metropolis-Hastings construye una cadena de Markov que converge a la distribución objetivo mediante propuestas locales y una regla de aceptación basada en densidades no normalizadas. Su fortaleza es la generalidad: cualquier propuesta válida permite muestrear correctamente. Sin embargo, su rendimiento depende críticamente de cómo de bien esas propuestas exploran el espacio, lo que conduce directamente a sus limitaciones.
  }
\end{frame}


\subsection{Limitations of Metropolis-Hastings}

\begin{frame}{Limitations of Metropolis-Hastings}
  
  \textbf{Random-walk proposals:}
  \[
    \theta' = \theta + \epsilon,
    \qquad \epsilon \sim \mathcal{N}(0, \sigma^2 I).
  \]
  
  \textbf{Problems:}
  \begin{itemize}
    \item Moves are local and uninformed.
    \item Slow exploration of the parameter space.
    \item High autocorrelation between samples.
    \item Very low acceptance rates for larger moves.
    \item Curse of dimensionality.
  \end{itemize}
  
  \vspace{0.2cm}
  
  \textbf{Result:} MH becomes inefficient in high dimensions or
  for correlated posteriors.
  
  \note{
    Las propuestas aleatorias (random walk) avanzan sin información geométrica, lo que provoca movimientos extremadamente locales, elevada autocorrelación y una exploración muy lenta, especialmente en dimensiones altas o distribuciones alargadas. Como consecuencia, el algoritmo necesita muchos más pasos para producir muestras útiles y su tasa de aceptación cae rápidamente cuando intentamos dar saltos más largos.
  }
\end{frame}



\section{Hamiltonian Dynamics}


\subsection{Introduction to Hamiltonian Dynamics}

\begin{frame}{Introduction to Hamiltonian Dynamics}
  
  \textbf{Idea:} Use physics to propose efficient moves in the parameter space.
  
  \begin{itemize}
    \item Treat parameters as a \textbf{position} vector $q$.
    \item Introduce an auxiliary \textbf{momentum} variable $p$.
    \item Define a \textbf{total energy} (Hamiltonian)
          \[
            H(q,p) = U(q) + K(p).
          \]
    \item The particle moves through the energy landscape following
          smooth, long-range trajectories.
  \end{itemize}
  
  \vspace{0.2cm}
  
  \textbf{Why this helps:} proposals follow the geometry of the target
  distribution, reducing random-walk behaviour.
  
  \note{
    Para superar las limitaciones del random walk, modelamos la exploración como el movimiento de una partícula en un paisaje de energía: los parámetros son la posición, añadimos un momento artificial y definimos un Hamiltoniano que combina energía potencial y cinética. Las trayectorias resultantes son suaves y de largo alcance, lo que permite recorrer regiones de alta probabilidad sin el comportamiento errático típico de Metropolis-Hastings.
  }
\end{frame}


\subsection{Potential and Kinetic Energy}

\begin{frame}{Potential and Kinetic Energy}
  
  \textbf{Potential energy:}
  \[
    U(q) = -\log \tilde{\pi}(q),
  \]
  where $\tilde{\pi}(q)$ is the unnormalised target density.
  
  \begin{itemize}
    \item High-probability regions $\Rightarrow$ low potential energy.
    \item The energy landscape encodes the geometry of the posterior.
  \end{itemize}
  
  \vspace{0.3cm}
  
  \textbf{Kinetic energy:}
  \[
    K(p) = \tfrac{1}{2} p^\top M^{-1} p,
    \qquad p \sim \mathcal{N}(0, M).
  \]
  
  \begin{itemize}
    \item Introduces momentum to help the sampler move through $q$-space.
    \item Mass matrix $M$ controls scaling and correlations.
  \end{itemize}
  
  \vspace{0.3cm}
  
  \textbf{Hamiltonian:} \quad $H(q,p) = U(q) + K(p)$.
  
  \note{
    La energía potencial se corresponde con el -logaritmo de la densidad objetivo no normalizada, por lo que las zonas más probables son “valles” del paisaje. La energía cinética proviene del momento, que introducimos de manera artificial para impulsar el movimiento. El uso de una matriz de masa permite adaptar la escala y correlaciones, capturando mejor la geometría de la distribución. La suma de ambas define el Hamiltoniano que determina la dinámica.
  }
\end{frame}


\subsection{Hamilton's Equations}

\begin{frame}{Hamilton's Equations}
  
  Given $H(q,p) = U(q) + K(p)$, the dynamics are defined by:
  
  \[
    \frac{dq}{dt} = \frac{\partial H}{\partial p},
    \qquad
    \frac{dp}{dt} = -\frac{\partial H}{\partial q}.
  \]
  
  \vspace{0.4cm}
  
  \textbf{Interpretation:}
  \begin{itemize}
    \item $q$ evolves according to the momentum.
    \item $p$ evolves according to the gradient of the potential.
    \item Total energy $H$ is approximately conserved.
  \end{itemize}
  
  \vspace{0.2cm}
  
  These trajectories form the basis of efficient proposals in HMC
  
  \note{
    Las ecuaciones de Hamilton describen cómo evoluciona el sistema: la posición cambia según el momento, y el momento cambia según el gradiente del potencial. Estas ecuaciones conservan la energía total y generan trayectorias que se ajustan al relieve de la distribución. En HMC, estas trayectorias se utilizan para proponer movimientos que son largos y coherentes y que tienen probabilidad alta de ser aceptados.
  }
\end{frame}



\section{Hamiltonian Monte Carlo}


\subsection{Leapfrog Integrator}

\begin{frame}{The Leapfrog Integrator}
  
  \textbf{Goal:} simulate Hamilton's equations numerically while preserving
  the key geometric properties of the dynamics.
  
  \vspace{0.15cm}
  
  \textbf{Given step size $\varepsilon$, one leapfrog step:}
  \begin{align*}
    p & \leftarrow p - \frac{\varepsilon}{2}\,\nabla U(q) 
      &                                                     & \text{(half momentum update)} \\
    q & \leftarrow q + \varepsilon\, M^{-1} p           
      &                                                     & \text{(full position update)} \\
    p & \leftarrow p - \frac{\varepsilon}{2}\,\nabla U(q) 
      &                                                     & \text{(half momentum update)}
  \end{align*}
  
  \vspace{0.15cm}
  
  \textbf{Why leapfrog?}
  \begin{itemize}
    \item \textbf{Reversible:} can run forwards or backwards.
    \item \textbf{Volume-preserving:} Jacobian determinant = 1.
    \item \textbf{Nearly energy-conserving:} for moderate step sizes.
  \end{itemize}
  
  \vspace{0.2cm}
  
  \textbf{Result:} long, stable trajectories = high-acceptance proposals.
  
  \note{
    Como no podemos resolver analíticamente las ecuaciones de Hamilton, utilizamos el integrador leapfrog, que alterna actualizaciones del momento y la posición. Este integrador es especial porque preserva volumen, es reversible y mantiene aproximadamente constante la energía, propiedades esenciales para que las propuestas sean válidas dentro del marco MCMC y para que la aceptación sea alta.
  }
\end{frame}


\subsection{The HMC Algorithm}

\begin{frame}{The HMC Algorithm}
  
  \textbf{Goal:} propose distant states with high acceptance using
  Hamiltonian trajectories.
  
  \vspace{0.3cm}
  
  \textbf{One HMC iteration:}
  \begin{enumerate}
    \item \textbf{Sample momentum}
          \[
            p \sim \mathcal{N}(0, M).
          \]
          
    \item \textbf{Simulate a trajectory}
          Run $L$ leapfrog steps with step size $\varepsilon$ to obtain
          $(q^*, p^*)$ from the initial $(q, p)$.
          
    \item \textbf{Metropolis acceptance step}
          \[
            \alpha = \min\left(1,\;
            \exp\left[-H(q^*, p^*) + H(q, p)\right]\right).
          \]
          Accept $q^*$ with probability $\alpha$, else keep $q$.
  \end{enumerate}
  
  \vspace{0.2cm}
  
  \textbf{Key idea:} leapfrog produces proposals with
  \emph{almost-conserved energy}, so acceptance rates are high.
  \vspace{1cm}
  
  \note{
    Un paso de HMC consiste en muestrear un momento nuevo, simular la dinámica con leapfrog durante varios pasos y luego aplicar una corrección Metropolis basada en el cambio de energía. Como la energía apenas varía durante la trayectoria, la probabilidad de aceptar el nuevo punto es muy elevada, lo que permite movimientos largos y naturales.
  }
\end{frame}


\subsection{Why HMC Works Well}

\begin{frame}{Why HMC Works Well}
  
  \textbf{Key advantages:}
  \begin{itemize}
    \item \textbf{Informed, gradient-driven proposals:} by following the
          geometry of $\log \tilde{\pi}(q)$, HMC avoids random walks and
          therefore scales much better to high-dimensional spaces
    \item \textbf{Statistical Independence:} leapfrog integration produces
          coherent paths through the parameter space.
    \item \textbf{High acceptance rates:} energy is almost conserved,
          so HMC rejections are rare.
    \item \textbf{Low autocorrelation:} successive samples are less dependent,
          improving effective sample size.
  \end{itemize}
  
  \vspace{0.2cm}
  
  \textbf{Result:} HMC explores complex posteriors much more efficiently
  than standard Metropolis-Hastings.
  
  \note{
    HMC utiliza el gradiente de la densidad para desplazarse de forma informada y evitar el comportamiento de random walk. Esto se traduce en baja autocorrelación, mayor independencia entre muestras y tasas de aceptación muy superiores. Además, estas propiedades escalan mucho mejor a dimensiones más altas, haciendo que HMC sea particularmente eficaz donde Metropolis-Hastings es cada vez peor.
  }
\end{frame}



\section{Experiments}


\subsection{Sampling from Gaussian Mixtures}

\begin{frame}{HMC Trajectories in a Gaussian Mixture Model}
  \centering
  \includegraphics[width=0.6\textwidth]{../images/hmc-trajectories.png}
  
  \note{
    En mezclas de Gaussianas, las trayectorias de HMC siguen curvas amplias que conectan regiones de alta densidad sin quedar atrapadas en un único modo. Las visualizaciones muestran cómo el sampler aprovecha la estructura del paisaje para moverse de manera natural entre componentes. 
  }
  
\end{frame}

\begin{frame}{HMC vs Metropolis-Hastings: Sample Comparison}
  \begin{columns}[T]
    \begin{column}{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../images/hmc-samples.png}\\[4pt]
      \small\textbf{Hamiltonian Monte Carlo}
    \end{column}
    \begin{column}{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../images/metropolis-samples.png}\\[4pt]
      \small\textbf{Metropolis Hastings}
    \end{column}
  \end{columns}
  
  \vspace{0.5cm}
  \note{
    La comparación de muestras revela claramente que HMC se distribuyé correctamente entre los modos. En contraste, MH tiende a concentrarse en una sola región y explora de forma mucho más limitada. En aplicaciones donde buscamos obtener muestras plausibles y bien dispersas con pocos pasos (más que reconstruir la distribución completa) HMC es mucho mejor opción.
  }
  
\end{frame}

\begin{frame}{MMD$^2$ Comparison}
  
  % --- TOP ROW ---
  \begin{columns}[T]
    
    % LEFT: Plot
    \begin{column}{0.50\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../images/mmd2-comparison-better.png}
    \end{column}
    
    % RIGHT: First two bullet points
    \begin{column}{0.43\textwidth}
      \textbf{Key observations:}
      \begin{itemize}
        \item \textbf{HMC converges quickly:} near-zero MMD$^2$ after $\sim$100 samples.
        \item \textbf{MH converges slowly:} needs several hundred samples.
      \end{itemize}
    \end{column}
    
  \end{columns}
  
  \vspace{0.25cm}
  
  % --- BOTTOM FULL-WIDTH ROW ---
  \begin{minipage}{0.95\textwidth}
    \begin{itemize}
      \item \textbf{Practical implication:} in many applications, the goal is to
            obtain plausible samples rather than to perfectly reconstruct the 
            full target distribution.
      \item \textbf{Result:} HMC produces high-quality samples with far fewer
            iterations, making it a better choice than standard 
            Metropolis-Hastings.
    \end{itemize}
  \end{minipage}
  
  \note{
    El análisis mediante MMD\^2 muestra que HMC alcanza rápidamente una distancia mínima respecto a la distribución real, mientras que MH requiere muchos más pasos para acercarse. Esto confirma que, cuando el objetivo práctico es generar buenas muestras en pocas iteraciones, HMC ofrece resultados de mucha mayor calidad sin requerir una cadena larga.
  }
\end{frame}


\subsection{Sampling from Neal's Funnel}

\begin{frame}{HMC vs Metropolis-Hastings: Sample Comparison}
  \begin{columns}[T]
    \begin{column}{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../images/nf-hmc-samples.png}\\[4pt]
      \small\textbf{Hamiltonian Monte Carlo}
    \end{column}
    \begin{column}{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../images/nf-metropolis-samples.png}\\[4pt]
      \small\textbf{Metropolis Hastings}
    \end{column}
  \end{columns}
  
  \vspace{0.5cm}
  
  \note{
    El embudo de Neal es un ejemplo típico donde MH falla debido a la fuerte dependencia entre dimensiones y la geometría estrecha del embudo. HMC, en cambio, sigue las trayectorias inducidas por el gradiente y es capaz de atravesar la estructura de la distribución con mucha mayor estabilidad, produciendo muestras representativas incluso en las zonas de mayor dificultad geométrica.
  }
\end{frame}

\begin{frame}{MMD$^2$ Comparison}
  
  % --- TOP ROW ---
  \begin{columns}[T]
    
    % LEFT: Plot
    \begin{column}{\textwidth}
      \centering
      \includegraphics[width=0.8\textwidth]{../images/nf-mmd2-full-comparison.png}
    \end{column}
    
  \end{columns}
  
  \vspace{0.25cm}
  
  % --- BOTTOM FULL-WIDTH ROW ---
  \begin{minipage}{0.95\textwidth}
    \begin{itemize}
      \item \textbf{Practical implication:} Hamiltonian Monte Carlo sampling can be much more effective in high-dimensional distributions.
      \item \textbf{Result:} HMC bypasses the curse of dimensionality and its acceptance rarte is much higher, making it a faster sampling algorithm.
    \end{itemize}
  \end{minipage}
  
  \note{
    En dimensiones elevadas, los resultados son más claros: HMC mantiene una tasa de aceptación cercana al 100\% y su MMD\^2 cae rápidamente, mientras que MH sufre la maldición de la dimensionalidad y muestra unaa menos aceptación y calidad de muestreo. Esto demuestra que HMC no solo es más rápido, sino también más robusto cuando la complejidad aumenta.
  }
  
\end{frame}


% ---- Final Thank You ----
\begin{frame}
  \centering
  \Large \textbf{Thank you for your attention!}\\[8pt]
  \normalsize Questions?
  
  \vspace{2.5cm}
  
  Code:
  
  \url{https://github.com/IgnacioBayon/hamilton-monte-carlo}
\end{frame}



\end{document}