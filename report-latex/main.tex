\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{booktabs}

\usetheme{Madrid}
\usecolortheme{default}

%------------------------------------------------------------
%This block of code defines the information to appear in the
%Title page
\title[HMC]
{Hamiltonian Monte Carlo}

\subtitle{MSc in AI --- Probabilistic Methods}

\author[PM]{%
  \texorpdfstring{
    Ignacio Bayón Jimenez-Ugarte\newline
    Sergio Herreros Pérez\newline
    Adrián López-Lanchares Echezarreta  
  }{Ignacio Bayón Jimenez-Ugarte, Sergio Herreros Pérez, Adrián López-Lanchares Echezarreta}
}

\institute[ICAI]{Universidad Pontificia Comillas, ICAI}

\date[08/29/2025]{08/29/2025}

\logo{\includegraphics[height=1cm]{images/Logo_Comillas}}

%End of title page configuration block
%------------------------------------------------------------



%------------------------------------------------------------
%The next block of commands puts the table of contents at the 
%beginning of each section and highlights the current section:

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[currentsection]
  \end{frame}
}
%------------------------------------------------------------


\begin{document}

% ---------- Title ----------
\begin{frame}\titlepage\end{frame}


% ===================== BODY =====================
\section{Motivation and Background}


\subsection{The Sampling Problem}

\begin{frame}{The Sampling Problem}
  
  \textbf{Goal:} draw samples from a target posterior distribution
  \[
    p(\theta \mid \text{data})
    = \frac{p(\text{data} \mid \theta)\, p(\theta)}{Z},
    \quad
    Z = \int p(\text{data} \mid \theta)\, p(\theta)\, d\theta.
  \]
  
  \vspace{0.3cm}
  
  \textbf{Issue:} the normalising constant $Z$ is often intractable.
  
  \vspace{0.3cm}
  
  \textbf{Workaround in MCMC:}
  \begin{itemize}
    \item We only need the unnormalised density
          \[
            \tilde{\pi}(\theta)
            = p(\text{data} \mid \theta)\, p(\theta)
            \propto p(\theta \mid \text{data}).
          \]
    \item MCMC algorithms use ratios such as
          \(
          \tilde{\pi}(\theta') / \tilde{\pi}(\theta)
          \),
          where $Z$ cancels out.
  \end{itemize}
  
  \vspace{0.2cm}
  
  \textbf{Conclusion:} We can sample from the posterior using only the
  \emph{unnormalised} density $\tilde{\pi}(\theta)$.
\end{frame}


\subsection{The Metropolis-Hastings Algorithm}

\begin{frame}{The Metropolis-Hastings Algorithm}
  
  \textbf{Goal:} construct a Markov chain whose stationary distribution is the
  target posterior $\pi(\theta)$ (known up to a normalising constant).
  
  \vspace{0.2cm}
  
  \textbf{Algorithm (one iteration):}
  \begin{enumerate}
    \item At current state $\theta$, draw a proposal
          \[
            \theta' \sim q(\theta' \mid \theta) \quad (q\text{: proposal distribution}).
          \]
    \item Compute the acceptance probability
          \[
            \alpha =
            \min\left(
            1,\;
            \frac{\tilde{\pi}(\theta')\, q(\theta \mid \theta')}
              {\tilde{\pi}(\theta)\, q(\theta' \mid \theta)}
            \right).
          \]
    \item With probability $\alpha$, move to $\theta'$;
          otherwise, stay at $\theta$.
  \end{enumerate}
  
  \vspace{0.2cm}
  
  \textbf{Key properties:}
  \begin{itemize}
    \item Ensures $\pi(\theta)$ is the stationary distribution.
    \item Works with the \emph{unnormalised} density $\tilde{\pi}(\theta)$.
    \item Very general: any proposal $q$ is allowed (almost).
  \end{itemize}
  
\end{frame}


\subsection{Limitations of Metropolis-Hastings}

\begin{frame}{Limitations of Metropolis-Hastings}
  
  \textbf{Random-walk proposals:}
  \[
    \theta' = \theta + \epsilon,
    \qquad \epsilon \sim \mathcal{N}(0, \sigma^2 I).
  \]
  
  \textbf{Problems:}
  \begin{itemize}
    \item Moves are local and uninformed.
    \item Slow exploration of the parameter space.
    \item High autocorrelation between samples.
    \item Very low acceptance rates for larger moves.
    \item Curse of dimensionality.
  \end{itemize}
  
  \vspace{0.2cm}
  
  \textbf{Result:} MH becomes inefficient in high dimensions or
  for correlated posteriors.
\end{frame}



\section{Hamiltonian Dynamics}


\subsection{Introduction to Hamiltonian Dynamics}

\begin{frame}{Introduction to Hamiltonian Dynamics}
  
  \textbf{Idea:} Use physics to propose efficient moves in the parameter space.
  
  \begin{itemize}
    \item Treat parameters as a \textbf{position} vector $q$.
    \item Introduce an auxiliary \textbf{momentum} variable $p$.
    \item Define a \textbf{total energy} (Hamiltonian)
          \[
            H(q,p) = U(q) + K(p).
          \]
    \item The particle moves through the energy landscape following
          smooth, long-range trajectories.
  \end{itemize}
  
  \vspace{0.2cm}
  
  \textbf{Why this helps:} proposals follow the geometry of the target
  distribution, reducing random-walk behaviour.
\end{frame}


\subsection{Potential and Kinetic Energy}

\begin{frame}{Potential and Kinetic Energy}
  
  \textbf{Potential energy:}
  \[
    U(q) = -\log \tilde{\pi}(q),
  \]
  where $\tilde{\pi}(q)$ is the unnormalised target density.
  
  \begin{itemize}
    \item High-probability regions $\Rightarrow$ low potential energy.
    \item The energy landscape encodes the geometry of the posterior.
  \end{itemize}
  
  \vspace{0.3cm}
  
  \textbf{Kinetic energy:}
  \[
    K(p) = \tfrac{1}{2} p^\top M^{-1} p,
    \qquad p \sim \mathcal{N}(0, M).
  \]
  
  \begin{itemize}
    \item Introduces momentum to help the sampler move through $q$-space.
    \item Mass matrix $M$ controls scaling and correlations.
  \end{itemize}
  
  \vspace{0.3cm}
  
  \textbf{Hamiltonian:} \quad $H(q,p) = U(q) + K(p)$.
\end{frame}


\subsection{Hamilton's Equations}

\begin{frame}{Hamilton's Equations}
  
  Given $H(q,p) = U(q) + K(p)$, the dynamics are defined by:
  
  \[
    \frac{dq}{dt} = \frac{\partial H}{\partial p},
    \qquad
    \frac{dp}{dt} = -\frac{\partial H}{\partial q}.
  \]
  
  \vspace{0.4cm}
  
  \textbf{Interpretation:}
  \begin{itemize}
    \item $q$ evolves according to the momentum.
    \item $p$ evolves according to the gradient of the potential.
    \item Total energy $H$ is approximately conserved.
  \end{itemize}
  
  \vspace{0.2cm}
  
  These trajectories form the basis of efficient proposals in HMC
\end{frame}



\section{Hamiltonian Monte Carlo}


\subsection{Leapfrog Integrator}

\begin{frame}{The Leapfrog Integrator}
  
  \textbf{Goal:} simulate Hamilton's equations numerically while preserving
  the key geometric properties of the dynamics.
  
  \vspace{0.15cm}
  
  \textbf{Given step size $\varepsilon$, one leapfrog step:}
  \begin{align*}
    p & \leftarrow p - \frac{\varepsilon}{2}\,\nabla U(q) 
      &                                                     & \text{(half momentum update)} \\
    q & \leftarrow q + \varepsilon\, M^{-1} p           
      &                                                     & \text{(full position update)} \\
    p & \leftarrow p - \frac{\varepsilon}{2}\,\nabla U(q) 
      &                                                     & \text{(half momentum update)}
  \end{align*}
  
  \vspace{0.15cm}
  
  \textbf{Why leapfrog?}
  \begin{itemize}
    \item \textbf{Reversible:} can run forwards or backwards.
    \item \textbf{Volume-preserving:} Jacobian determinant = 1.
    \item \textbf{Nearly energy-conserving:} for moderate step sizes.
  \end{itemize}
  
  \vspace{0.2cm}
  
  \textbf{Result:} long, stable trajectories = high-acceptance proposals.
\end{frame}


\subsection{The HMC Algorithm}

\begin{frame}{The HMC Algorithm}
  
  \textbf{Goal:} propose distant states with high acceptance using
  Hamiltonian trajectories.
  
  \vspace{0.3cm}
  
  \textbf{One HMC iteration:}
  \begin{enumerate}
    \item \textbf{Sample momentum}
          \[
            p \sim \mathcal{N}(0, M).
          \]
          
    \item \textbf{Simulate a trajectory}
          Run $L$ leapfrog steps with step size $\varepsilon$ to obtain
          $(q^*, p^*)$ from the initial $(q, p)$.
          
    \item \textbf{Metropolis acceptance step}
          \[
            \alpha = \min\left(1,\;
            \exp\left[-H(q^*, p^*) + H(q, p)\right]\right).
          \]
          Accept $q^*$ with probability $\alpha$, else keep $q$.
  \end{enumerate}
  
  \vspace{0.2cm}
  
  \textbf{Key idea:} leapfrog produces proposals with
  \emph{almost-conserved energy}, so acceptance rates are high.
  \vspace{1cm}
  
\end{frame}


\subsection{Why HMC Works Well}

\begin{frame}{Why HMC Works Well}
  
  \textbf{Key advantages:}
  \begin{itemize}
    \item \textbf{Informed, gradient-driven proposals:} by following the
          geometry of $\log \tilde{\pi}(q)$, HMC avoids random walks and
          therefore scales much better to high-dimensional spaces
    \item \textbf{Statistical Independence:} leapfrog integration produces
          coherent paths through the parameter space.
    \item \textbf{High acceptance rates:} energy is almost conserved,
          so HMC rejections are rare.
    \item \textbf{Low autocorrelation:} successive samples are less dependent,
          improving effective sample size.
  \end{itemize}
  
  \vspace{0.2cm}
  
  \textbf{Result:} HMC explores complex posteriors much more efficiently
  than standard Metropolis-Hastings.
\end{frame}



\section{Experiments}


\subsection{Sampling from Gaussian Mixtures}

\begin{frame}{HMC Trajectories in a Gaussian Mixture}
  \centering
  \includegraphics[width=0.6\textwidth]{../images/hmc-trajectories.png}
\end{frame}

\begin{frame}{HMC vs Metropolis-Hastings: Sample Comparison}
  \begin{columns}[T]
    \begin{column}{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../images/hmc-samples.png}\\[4pt]
      \small\textbf{Hamiltonian Monte Carlo}
    \end{column}
    \begin{column}{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../images/metropolis-samples.png}\\[4pt]
      \small\textbf{Metropolis Hastings}
    \end{column}
  \end{columns}

  \vspace{0.5cm}
\end{frame}

\begin{frame}{MMD$^2$ Comparison}
  
  % --- TOP ROW ---
  \begin{columns}[T]
    
    % LEFT: Plot
    \begin{column}{0.50\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../images/mmd2-comparison-better.png}
    \end{column}
    
    % RIGHT: First two bullet points
    \begin{column}{0.43\textwidth}
      \textbf{Key observations:}
      \begin{itemize}
        \item \textbf{HMC converges quickly:} near-zero MMD$^2$ after $\sim$100 samples.
        \item \textbf{MH converges slowly:} needs several hundred samples.
      \end{itemize}
    \end{column}
    
  \end{columns}
  
  \vspace{0.25cm}
  
  % --- BOTTOM FULL-WIDTH ROW ---
  \begin{minipage}{0.95\textwidth}
    \begin{itemize}
      \item \textbf{Practical implication:} in many applications, the goal is to
            obtain plausible samples rather than to perfectly reconstruct the 
            full target distribution.
      \item \textbf{Result:} HMC produces high-quality samples with far fewer
            iterations, making it a better choice than standard 
            Metropolis-Hastings.
    \end{itemize}
  \end{minipage}
  
\end{frame}


\subsection{Sampling from Neal's Funnel}

\begin{frame}{HMC vs Metropolis-Hastings: Sample Comparison}
  \begin{columns}[T]
    \begin{column}{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../images/nf-hmc-samples.png}\\[4pt]
      \small\textbf{Hamiltonian Monte Carlo}
    \end{column}
    \begin{column}{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../images/nf-metropolis-samples.png}\\[4pt]
      \small\textbf{Metropolis Hastings}
    \end{column}
  \end{columns}

  \vspace{0.5cm}
\end{frame}

\begin{frame}{MMD$^2$ Comparison}
  
  % --- TOP ROW ---
  \begin{columns}[T]
    
    % LEFT: Plot
    \begin{column}{\textwidth}
      \centering
      \includegraphics[width=0.8\textwidth]{../images/nf-mmd2-full-comparison.png}
    \end{column}
    
  \end{columns}
  
  \vspace{0.25cm}
  
  % --- BOTTOM FULL-WIDTH ROW ---
  \begin{minipage}{0.95\textwidth}
    \begin{itemize}
      \item \textbf{Practical implication:} Hamiltonian Monte Carlo sampling can be much more effective in high-dimensional distributions.
      \item \textbf{Result:} HMC bypasses the curse of dimensionality and its acceptance rarte is much higher, making it a faster sampling algorithm.
    \end{itemize}
  \end{minipage}
  
\end{frame}


% ---- Final Thank You ----
\begin{frame}
  \centering
  \Large \textbf{Thank you for your attention!}\\[8pt]
  \normalsize Questions?
\end{frame}



\end{document}